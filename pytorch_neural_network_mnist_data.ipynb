{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">using cuda: NVIDIA GeForce RTX <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3080</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "using cuda: NVIDIA GeForce RTX \u001b[1;36m3080\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "# if yes, set default tensor type to cuda\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    print(\"using cuda:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">device</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'cuda'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, inodes, hnodes, onodes, learning_rate):\n",
    "        # call the base class's initialisation too\n",
    "        super().__init__()\n",
    "        \n",
    "        # dimensions\n",
    "        self.inodes, self.hnodes, self.onodes = inodes, hnodes, onodes\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # define the layers and their sizes, turn off bias\n",
    "        self.linear_ih = nn.Linear(inodes, hnodes, bias=False)\n",
    "        self.linear_ho = nn.Linear(hnodes, onodes, bias=False)\n",
    "        \n",
    "        # define activation function\n",
    "        self.activation = nn.Sigmoid()\n",
    "        # define inverse activation function\n",
    "        self.inverse_activation_function = torch.logit\n",
    "        \n",
    "        # create error function\n",
    "        # self.error_function = torch.nn.MSELoss(size_average=False)  # size_average is deprecated\n",
    "        self.error_function = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "        # create optimiser, using simple stochastic gradient descent\n",
    "        self.optimiser = torch.optim.SGD(self.parameters(), self.lr)\n",
    "\n",
    "        # use GPU if it is available\n",
    "        self.ftensor = torch.cuda.FloatTensor if device.type=='cuda' else torch.FloatTensor\n",
    "            \n",
    "    \n",
    "    def forward(self, inputs_list):\n",
    "        # convert list to a 2-D FloatTensor then wrap in Variable \n",
    "        inputs = Variable(self.ftensor(inputs_list).view(1, self.inodes))\n",
    "            \n",
    "        # combine input layer signals into hidden layer\n",
    "        hidden_inputs = self.linear_ih(inputs)\n",
    "        # apply sigmiod activation function\n",
    "        hidden_outputs = self.activation(hidden_inputs)\n",
    "        \n",
    "        # combine hidden layer signals into output layer\n",
    "        final_inputs = self.linear_ho(hidden_outputs)\n",
    "        # apply sigmiod activation function\n",
    "        final_outputs = self.activation(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "\n",
    "    \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # calculate the output of the network\n",
    "        output = self.forward(inputs_list)\n",
    "\n",
    "        # create a Variable out of the target vector, doesn't need gradients calculated\n",
    "        target_variable = Variable(self.ftensor(targets_list).view(1, self.onodes), requires_grad=False)\n",
    "        \n",
    "        # calculate error\n",
    "        loss = self.error_function(output, target_variable)\n",
    "        # print(loss.data[0])\n",
    "\n",
    "        # zero gradients, perform a backward pass, and update the weights.\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        \n",
    "        \n",
    "    def backquery(self, targets_list):\n",
    "        \"\"\"backquery the neural network.\"\"\"\n",
    "        def _scale(signal):\n",
    "            # scale signal to 0.01 to 0.99\n",
    "            signal -= signal.min().item()\n",
    "            signal /= signal.max().item()\n",
    "            signal *= 0.98\n",
    "            signal += 0.01\n",
    "            return signal\n",
    "        \n",
    "        # create a Variable out of the target vector, doesn't need gradients calculated\n",
    "        target_variable = Variable(self.ftensor(targets_list).view(1, self.onodes), requires_grad=False).T\n",
    "        \n",
    "        # calculate the signal into the final output layer\n",
    "        final_inputs = self.inverse_activation_function(target_variable)\n",
    "        # calculate the signal out of the hidden layer and scale them back to 0.01 to 0.99\n",
    "        hidden_outputs = _scale(torch.matmul(self.linear_ho.weight.T, final_inputs))\n",
    "        \n",
    "        # calculate the signal into the hidden layer\n",
    "        hidden_inputs = self.inverse_activation_function(hidden_outputs)\n",
    "        # calculate the signal out of the input layer and scale them back to 0.01 to .99\n",
    "        inputs = _scale(torch.matmul(self.linear_ih.weight.T, hidden_inputs))\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes, hidden_nodes, output_nodes = 784, 200, 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# create instance of neural network\n",
    "n = Classifier(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "# move neural network to the GPU\n",
    "if device.type == 'cuda':\n",
    "    n.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the mnist training data CSV file into a list\n",
    "training_data_file = open(\"./mnist_dataset/mnist_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34min 2s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1 -c\n",
    "\n",
    "# train the neural network\n",
    "\n",
    "# epochs is the number of times the training data set is used for training\n",
    "epochs = 10  # best\n",
    "# epochs = 5\n",
    "# epochs = 1\n",
    "\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        \n",
    "        ## create rotated variations\n",
    "        # rotated anticlockwise by x degrees\n",
    "        x = 10\n",
    "        inputs_plusx_img = scipy.ndimage.interpolation.rotate(inputs.reshape(28,28), x, cval=0.01, order=1, reshape=False)\n",
    "        n.train(inputs_plusx_img.reshape(784), targets)\n",
    "        # rotated clockwise by x degrees\n",
    "        inputs_minusx_img = scipy.ndimage.interpolation.rotate(inputs.reshape(28,28), -x, cval=0.01, order=1, reshape=False)\n",
    "        n.train(inputs_minusx_img.reshape(784), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load the mnist test data CSV file into a list\n",
    "test_data_file = open(\"./mnist_dataset/mnist_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    # query the network\n",
    "    outputs = n.forward(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    m, label = outputs.max(1)\n",
    "    # print(m, label)\n",
    "    # append correct or incorrect to list\n",
    "    # need to extract from pytorch tensor via numpy to compare to python integer\n",
    "    # if (numpy.asarray(label)[0] == correct_label):\n",
    "    if (label.item() == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">performance = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9809</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "performance = \u001b[1;36m0.9809\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print(f\"performance = {scorecard_array.sum()/scorecard_array.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test backquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">matplotlib.image.AxesImage</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7f3332f25c10</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mmatplotlib.image.AxesImage\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f3332f25c10\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Figure</span><span style=\"color: #000000; text-decoration-color: #000000\"> size 432x288 with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> Axes</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mFigure\u001b[0m\u001b[39m size 432x288 with \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m Axes\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = 9\n",
    "targets = numpy.zeros(output_nodes) + 0.01\n",
    "targets[label] = 0.99\n",
    "\n",
    "image_data = n.backquery(targets).cpu().detach().numpy().reshape(28, 28)\n",
    "plt.imshow(image_data, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Classifier</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>linear_ih<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">784</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>linear_ho<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>activation<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sigmoid</span><span style=\"font-weight: bold\">()</span>\n",
       "  <span style=\"font-weight: bold\">(</span>error_function<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MSELoss</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35mClassifier\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlinear_ih\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m784\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m200\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlinear_ho\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m200\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m10\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mactivation\u001b[1m)\u001b[0m: \u001b[1;35mSigmoid\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0merror_function\u001b[1m)\u001b[0m: \u001b[1;35mMSELoss\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
